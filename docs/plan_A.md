# 概要
- キーボードへの教師なし音響サイドチャネル攻撃の高精度化を目的とする．
- 既存研究のHMMをTransformerに置き換え，文脈的補完を可能にする攻撃モデルを提案する．

---
# 構成
## Transformerの事前学習
### モデル構造
- 自己回帰型のDecoder-only Transformer（GPT型構造）
- 入力文字列から次の1文字を予測するタスクとして学習
- トークンは**1文字単位**
- モデル構成の例：
  - 入力層：文字埋め込み（文字 → トークンID → トークン）
  - 中間層：多層自己注意ブロック（6～12層程度）
  - 出力層：ソフトマックスで次文字確率を出力


### データセット
- 英語テキストコーパス（Openwebtextなど）
- 文字単位に分割したシーケンスを入力として使用
- コーパスサイズは10⁷〜10⁸文字程度


### 学習
- 損失関数：CrossEntropy関数
- 最適化：AdamW
- シーケンス長：64~256文字（固定長）
  - モデルが一度に扱う文字数のこと
  - シーケンス長よりも短い入力はPaddingで対応
- バッチサイズ：メモリ使用量を見て調整


---
## 攻撃モデル
### a. クラスタリング
1. 打鍵音を録音
2. 録音波形から打鍵部分をセグメント化
3. 打鍵波形を特徴量変換
4. 打鍵特徴量をクラスタリング　<- ここまでは従来手法と同じ


### b. 文章復元

1. **クラスタ番号と文字の対応表を初期化**
    - 各クラスタ番号 \( c_i \) に対して，どの文字 \( y_j \) に対応するかの確率分布を初期化する．
    - これを **対応確率行列** と呼ぶ（正式な名前はない）：
      \[
      M_{ij} = P(y_j \mid c_i)
      \]
    - スペースキーなど一部のクラスタは初期化時に高信頼で割り当てる．
<br>

2. **クラスタ列を埋め込み表現に変換**（ここが肝）

    - 各クラスタ \( c_i \) は「どの文字であるか」が確率的に不確定．
    - 事前学習した文字埋め込み行列 \( W_E \in \mathbb{R}^{|V| \times d} \)（文字→ベクトル）を用いて，
      各クラスタを曖昧な埋め込みベクトルに変換する：
      \[
      E(c_i) = \sum_j P(y_j \mid c_i) \cdot W_E[y_j]
      \]
    - 直感的には：
      - 「クラスタ番号は a かもしれないし z かもしれない」 → a と z の中間的なベクトル
      - 複数の文字候補を重み付き加算した曖昧な表現

    - 埋め込み系列：
      \[
      \mathbf{E} = [E(c_1), E(c_2), \dots, E(c_T)]
      \]
      が Transformer の入力となる．
<br>


3. **埋め込み表現から文章を推定**

    - 事前学習済み Transformer は，**過去の文字列から次の1文字を予測**するように学習
    - 一般的な使い方なら，時刻t以前の文字列を入力して時刻tの文字確率を得る：
      \[
      P_{\text{Trans}}(y_t \mid y_{<t})
      \]
    - ここでは，クラスタ列の埋め込みを文脈として入力し，各位置で次に来そうな文字の確率分布を出力：
      \[
      P_{\text{Trans}}(y_t \mid c_{<t})
      \]
    - 学習時と攻撃時で入力形式と場所が違うことに注意．攻撃時は計算した埋め込みを，埋め込み変換層をスキップしてAttention層に直接入力する．
      ```Python
      # 既存モデルの埋め込み層を取得
      embedding_layer = model.transformer.wte

      # 攻撃時に曖昧埋め込みを作成
      E = torch.matmul(M, embedding_layer.weight)  # M: (num_clusters, |V|)

      # Transformerを直接呼ぶ
      outputs = model.transformer(inputs_embeds=E)

      ```
    - シーケンス長が 32 の場合，最初の 1〜32 文字分は Padding で対応し，文頭では Transformer が「英文の始まりとして自然な文字（例：大文字や ‘T’, ‘A’）」を高確率で出す傾向がある（はず）．
<br>


4. **推定文章とクラスタ列をもとに対応確率行列を更新**

    - 各クラスタ \( c_i \) に対し，音響モデル由来の確率 \( M_{ij}^{(\text{old})}=P(y_j \mid c_i) \) と，Transformer出力の文脈確率 \( P_{\text{Trans}}(y_j \mid \text{context}) \) を組み合わせる
    - 文全体の出力をクラスタ単位に平均化（推定文章で対応確率行列を構成するイメージ）：
      \[
      R_{ij} = \frac{1}{N_i}\sum_{t:c_t=i} P_{\text{Trans}}(y_j \mid \text{context}_t)
      \]
    - 更新：
      \[
      M^{(\text{new})}_{ij} \propto M^{(\text{old})}_{ij}\, R_{ij}
      \]
    - 確率分布になるように正規化：
      \[
      M_{ij}^{(\text{new})} =
      \frac{M_{ij}^{(\text{old})}\,R_{ij}}
      {\sum_k M_{ik}^{(\text{old})}\,R_{ik}}
      \]
    - これにより，「音響的に近い」＋「文脈的に自然」な文字が強化される．
    - [詳細](./temp.md)
<br>

5. **反復更新**

    - ステップ (2)〜(4) を繰り返すことで：
      - 対応確率行列 \( M \) は文脈的に整合するように収束する．
      - Transformer が文法・綴りの整合性を担保する。
    - 最終的に得られる対応表 \( M^* \) に基づき，
      クラスタ系列から自然な英語文章を復元する。
<br>



# その他
## 改良アイデア
- スペースキーがある程度検出できれば，文章中の"a", "the"などは推定できるのでは？対応確率行列の初期化に有効
- b.の2~4において，繰り返しの段階でDECも更新できるのでは？
  - 半教師的にラベルに合わせて分離を最適化