# 概要
- キーボードへの教師なし音響サイドチャネル攻撃の高精度化を目的とする．
- 既存研究のHMMを**Encoder–Decoder型Transformer**に置き換え，文全体の整合性を考慮した文章復元を行う攻撃モデルを提案する．
- クラスタ誤差（多対一・一対多）やスペースの高信頼性を活かしたロバストなモデルを設計．

---

# 構成
## Transformerの事前学習
### モデル構造
- **Encoder–Decoder型Transformer**（例：T5, BART）
- 入力：ランダムに置換されたクラスタID列（数列）
- 出力：元の英文（文字列）
- トークンは**1文字単位**
- 構造概要：
  - Encoder：数列入力を埋め込み・自己注意で符号化
  - Decoder：Encoder出力と自己回帰的注意により文を生成
  - 出力層：Softmaxによる次文字予測

### データセット
- 英語コーパス（Openwebtextなど）
- 各文を文字単位に分割し，ランダムなクラスタ番号列にマッピングして入力
- **ランダム割り当ては文ごとに変化**
  - 同じ文字が文によって異なるIDに対応する
  - クラスタ数や対応パターンも変化（1対多・多対一を含む）
- 学習時には**クラスタ誤差を模倣**
  - 例：EとRを混合，スペースを複数クラスタに分割，低確率ノイズを挿入
- この多様な生成過程により，モデルはクラスタ誤差に頑健な言語復元能力を獲得

### 学習
- 損失関数：クロスエントロピー（教師あり文字復元）
- 最適化：AdamW
- シーケンス長：64〜256文字
- バッチサイズ：メモリ容量に応じて調整
- 学習目的：
  - 「ランダム暗号化された数列から自然な文を復号」
  - 「曖昧なクラスタ空間でも言語整合性を再構成」

---

## 攻撃モデル
### a. 音響クラスタリング
1. 打鍵音を録音し，波形をセグメント化
2. 打鍵特徴量を抽出（例：MFCC, Melスペクトログラム）
3. クラスタリング（例：K-means, GMM, spectral clustering）
4. 各打鍵にクラスタ番号を割り当て  
   → クラスタ列 \( C = [c_1, c_2, \dots, c_T] \)

---

### b. 文章復元
1. **クラスタ埋め込みの初期化**
   - 学習済みTransformerの埋め込み行列 \( W_E \in \mathbb{R}^{|V| \times d} \) を使用
   - 各クラスタ番号を埋め込みベクトルに変換：
     \[
     E(c_i) = \text{Embedding}[c_i]
     \]
   - 既知の高信頼クラスタ（例：スペース）は初期値を固定

2. **Encoderへの入力**
   - クラスタ列 \( C \) を埋め込みベクトル系列 \( E(C) \) に変換
   - Encoderが系列全体を処理し，潜在表現 \( H \) を出力
     \[
     H = \text{Encoder}(E(C))
     \]
   - Encoderは全体文脈（スペース位置や近傍クラスタ）を考慮可能

3. **Decoderによる復号**
   - Decoderが \( H \) を条件に英文を自己回帰的に生成：
     \[
     P(y_t \mid y_{<t}, H)
     \]
   - Decoderは文全体の整合性を重視して出力を最尤化
   - 出力は1文字ずつ生成（文字単位トークン）

4. **自己整合的更新（軽量EM）**
   - 必要に応じてクラスタ埋め込みを微調整（E-step的手法）
   - Transformer出力の尤度と一致する方向にembeddingを更新：
     \[
     E(c_i) \leftarrow E(c_i) + \eta \nabla_{E(c_i)} \log P_\text{Trans}(y \mid C)
     \]
   - 数ステップで十分（完全なEMループは不要）

5. **最終出力**
   - モデルが直接，クラスタ列から整合した英語文章を生成
   - EMを使わなくても，クラスタ誤差に頑健な復号が可能

---

# その他
## 改良アイデア
- スペースキーが高信頼で検出可能なため，
  - “space, C1, space” パターンを利用して “a” などの冠詞を高精度に復元可能
  - Encoderが前後
